{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrameReader, SQLContext\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "spark\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: systemds in c:\\users\\usuario\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: py4j>=0.10.9 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from systemds) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.2.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from systemds) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from systemds) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.24.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from systemds) (2.28.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pandas>=1.2.2->systemds) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from pandas>=1.2.2->systemds) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.2->systemds) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests>=2.24.0->systemds) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests>=2.24.0->systemds) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests>=2.24.0->systemds) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from requests>=2.24.0->systemds) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install systemds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So the first thing we need to ensure is that we are on the latest version of SystemML, which is 1.3.0 (as of 20th March'19) Please use the code block below to check if you are already on 1.3.0 or higher. 1.3 contains a necessary fix, that's we are running against the SNAPSHOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('shake.parquet', <http.client.HTTPMessage at 0x24c89a5a160>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = 'https://github.com/LeonardoPalaciosPando1996/Database/blob/Master/shake.parquet?raw=true'\n",
    "filename = 'shake.parquet'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to read the sensor data and create a temporary query table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.parquet('shake.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-----+-----+\n",
      "|CLASS| SENSORID|    X|    Y|    Z|\n",
      "+-----+---------+-----+-----+-----+\n",
      "|    2| qqqqqqqq| 0.12| 0.12| 0.12|\n",
      "|    2|aUniqueID| 0.03| 0.03| 0.03|\n",
      "|    2| qqqqqqqq|-3.84|-3.84|-3.84|\n",
      "|    2| 12345678| -0.1| -0.1| -0.1|\n",
      "|    2| 12345678|-0.15|-0.15|-0.15|\n",
      "|    2| 12345678| 0.47| 0.47| 0.47|\n",
      "|    2| 12345678|-0.06|-0.06|-0.06|\n",
      "|    2| 12345678|-0.09|-0.09|-0.09|\n",
      "|    2| 12345678| 0.21| 0.21| 0.21|\n",
      "|    2| 12345678|-0.08|-0.08|-0.08|\n",
      "|    2| 12345678| 0.44| 0.44| 0.44|\n",
      "|    2|    gholi| 0.76| 0.76| 0.76|\n",
      "|    2|    gholi| 1.62| 1.62| 1.62|\n",
      "|    2|    gholi| 5.81| 5.81| 5.81|\n",
      "|    2| bcbcbcbc| 0.58| 0.58| 0.58|\n",
      "|    2| bcbcbcbc|-8.24|-8.24|-8.24|\n",
      "|    2| bcbcbcbc|-0.45|-0.45|-0.45|\n",
      "|    2| bcbcbcbc| 1.03| 1.03| 1.03|\n",
      "|    2|aUniqueID|-0.05|-0.05|-0.05|\n",
      "|    2| qqqqqqqq|-0.44|-0.44|-0.44|\n",
      "+-----+---------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use Apache SystemML to implement Discrete Fourier Transformation. This way all computation continues to happen on the Apache Spark cluster for advanced scalability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you’ve learned from the lecture, implementing Discrete Fourier Transformation in a linear algebra programming language is simple. Apache SystemML DML is such a language and as you can see the implementation is straightforward and doesn’t differ too much from the mathematical definition (Just note that the sum operator has been swapped with a vector dot product using the %*% syntax borrowed from R\n",
    "):\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1af0a78dc50bbf118ab6bd4c4dcc3c4ff8502223\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from systemds.context import SystemDSContext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def dft_systemds(signal,name):\n",
    "\n",
    "\n",
    "    with SystemDSContext(spark) as sds:\n",
    "        size = signal.count()  \n",
    "        signal = sds.from_numpy(signal.toPandas().to_numpy())\n",
    "        pi = sds.scalar(3.141592654)\n",
    "\n",
    "        n = sds.seq(0,size-1)\n",
    "        k = sds.seq(0,size-1)\n",
    "\n",
    "        M = (n @ (k.t())) * (2*pi/size)\n",
    "        \n",
    "        Xa = M.cos() @ signal\n",
    "        Xb = M.sin() @ signal\n",
    "\n",
    "        index = (list(map(lambda x: [x],np.array(range(0, size, 1)))))\n",
    "        DFT = np.hstack((index,Xa.cbind(Xb).compute()))\n",
    "        DFT_pdf = pd.DataFrame(DFT, columns=list([\"id\",name+'_sin',name+'_cos']))\n",
    "        DFT_df = spark.createDataFrame(DFT_pdf)\n",
    "        return DFT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to create a function which takes a single row Apache Spark data frame as argument (the one containing the accelerometer measurement time series for one axis) and returns the Fourier transformation of it. In addition, we are adding an index column for later joining all axis together and renaming the columns to appropriate names. The result of this function is an Apache Spark DataFrame containing the Fourier Transformation of its input in two columns. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to create individual DataFrames containing only a subset of the data. We filter simultaneously for accelerometer each sensor axis and one for each class. This means you’ll get 6 DataFrames. Please implement this using the relational API of DataFrames or SparkSQL. Please use class 1 and 2 and not 0 and 1. <h1><span style=\"color:red\">Please make sure that each DataFrame has only ONE colum (only the measurement, eg. not CLASS column)</span></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = spark.sql(\"SELECT X from df where class = 0\")\n",
    "y0 = spark.sql(\"SELECT Y from df where class = 0\")\n",
    "z0 = spark.sql(\"SELECT Z from df where class = 0\")\n",
    "x1 = spark.sql(\"SELECT X from df where class = 1\")\n",
    "y1 = spark.sql(\"SELECT Y from df where class = 1\")\n",
    "z1 = spark.sql(\"SELECT Z from df where class = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’ve created this cool DFT function before, we can just call it for each of the 6 DataFrames now. And since the result of this function call is a DataFrame again we can use the pyspark best practice in simply calling methods on it sequentially. So what we are doing is the following:\n",
    "\n",
    "- Calling DFT for each class and accelerometer sensor axis.\n",
    "- Joining them together on the ID column. \n",
    "- Re-adding a column containing the class index.\n",
    "- Stacking both Dataframes for each classes together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|  id|               x_sin|               x_cos|               y_sin|               y_cos|               z_sin|               z_cos|class|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 1.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 2.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 3.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 4.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 6.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 5.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 7.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 8.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 9.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "|10.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "|11.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "|13.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "|12.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n",
      "| 8.0|0.007264897147586408|0.050779226648936575|0.007264897147586408|0.050779226648936575|0.007264897147586408|0.050779226648936575|    1|\n",
      "| 0.0|               -0.13|                 0.0|               -0.13|                 0.0|               -0.13|                 0.0|    1|\n",
      "| 7.0| 0.04362147198699657| 0.04661273969005281| 0.04362147198699657| 0.04661273969005281| 0.04362147198699657| 0.04661273969005281|    1|\n",
      "| 1.0|-0.02887452407349862|0.026700581521828547|-0.02887452407349862|0.026700581521828547|-0.02887452407349862|0.026700581521828547|    1|\n",
      "| 4.0|0.010027156636270794|-0.03439553396608...|0.010027156636270794|-0.03439553396608...|0.010027156636270794|-0.03439553396608...|    1|\n",
      "| 3.0|-0.03997869778182...| 0.03172866159802132|-0.03997869778182...| 0.03172866159802132|-0.03997869778182...| 0.03172866159802132|    1|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_class_0 = dft_systemds(x0,'x') \\\n",
    "    .join(dft_systemds(y0,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemds(z0,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(0))\n",
    "    \n",
    "df_class_1 = dft_systemds(x1,'x') \\\n",
    "    .join(dft_systemds(y1,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemds(z1,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(1))\n",
    "\n",
    "df_dft = df_class_0.union(df_class_1)\n",
    "\n",
    "df_dft.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please create a VectorAssembler which consumes the newly created DFT columns and produces a column “features”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"x_sin\", \"x_cos\", \"y_sin\", \"y_cos\", \"z_sin\", \"z_cos\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please insatiate a classifier from the SparkML package and assign it to the classifier variable. Make sure to set the “class” column as target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GBTClassifier(labelCol=\"class\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train and evaluate…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  id|               x_sin|               x_cos|               y_sin|               y_cos|               z_sin|               z_cos|class|            features|       rawPrediction|         probability|prediction|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "| 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 1.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 2.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 3.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 4.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 6.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 5.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 7.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 8.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 9.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "|10.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "|11.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "|13.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "|12.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|[1.32590267922033...|[0.93412217565278...|       0.0|\n",
      "| 8.0|0.007264897147586408|0.050779226648936575|0.007264897147586408|0.050779226648936575|0.007264897147586408|0.050779226648936575|    1|[0.00726489714758...|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "| 0.0|               -0.13|                 0.0|               -0.13|                 0.0|               -0.13|                 0.0|    1|[-0.13,0.0,-0.13,...|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "| 7.0| 0.04362147198699657| 0.04661273969005281| 0.04362147198699657| 0.04661273969005281| 0.04362147198699657| 0.04661273969005281|    1|[0.04362147198699...|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "| 1.0|-0.02887452407349862|0.026700581521828547|-0.02887452407349862|0.026700581521828547|-0.02887452407349862|0.026700581521828547|    1|[-0.0288745240734...|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "| 4.0|0.010027156636270794|-0.03439553396608...|0.010027156636270794|-0.03439553396608...|0.010027156636270794|-0.03439553396608...|    1|[0.01002715663627...|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "| 3.0|-0.03997869778182...| 0.03172866159802132|-0.03997869778182...| 0.03172866159802132|-0.03997869778182...| 0.03172866159802132|    1|[-0.0399786977818...|[-1.3259026792203...|[0.06587782434721...|       1.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"class\")\n",
    "binEval.evaluate(prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you happy with the result? I’m happy with > 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = prediction.repartition(1)\n",
    "#prediction.write.json('a2_m4.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
